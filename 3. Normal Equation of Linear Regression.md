# Normal Equation of Linear Regression

## Resources

- [The Hundred-Page Machine Learning Book](http://themlbook.com/wiki/doku.php)
  - [Chapter 3 – Fundamental Algorithms](http://bit.ly/theMLbook-Chapter-3): Pages 3 to 6

## Overview of Topics Covered

- Linear Regression
  1. Problem Statement
  2. Solution

## Derivation of Normal Equation

1. Reduce the normal equation in the form 
   $$
   y=w^Tx
   $$
2. Define the error function as
   $$
   E=\frac{1}{N}\displaystyle{\sum_{i=1}^{i=N}f(x_i)-y_i}
   $$
3. Write error function in matrix form as
   $$
   E=\frac{1}{N}\|Xw-Y\|
   $$
   where $X,Y$ should be deduced by the reader.
4. Solve
   $$
   \frac{\partial{E}}{\partial{w}}=0
   $$
   for $w$ to get
   $$
   w=(X^TX)^{-1}X^TY
   $$
5. Refer the following to get help in derivation:
   - [prutor.ai](https://prutor.ai/normal-equation-in-linear-regression/) – To get help in formulation of $X,Y$
   - [mlwiki.org](mlwiki.org/index.php/Normal_Equation) – To get some extra details for the solution

## Questions to ponder upon

1. Why should the inverse of $X^TX$ exist?
   - [Solution Link](https://math.stackexchange.com/a/691836)
2. Find the problem in the following simplification:
   $$
   (X^TX)^{-1}=X^{-1}(X^T)^{-1}
   $$
3. Compute the time complexity to compute $w$ in the result derived above.
4. Apply SVD to $X$ and rewrite the normal form of linear regression. What is the time complexity to compute $w$ using this approach? Which one is better?
5. Add more questions here if you find any..

## C++ Implementation

- Include header files

  ```c++
  #include <iostream>
  #include <Eigen/Dense>
  #include <Eigen/Sparse>
  #include <utility>
  #include <string>
  #include <fstream>
  
  using namespace std;
  using namespace Eigen;
  ```

- Read dataset from path string `filepath`

  ```c++
  pair<MatrixXd, VectorXd> get_data(string& filepath)
  {
  	ifstream file(filepath);
  	int n, m;
  	file >> n >> m;
  	MatrixXd X(n, m + 1);
  	VectorXd Y(n);
  	for (int i = 0; i < n; ++i)
  	{
  		for (int j = 0; j < m; ++j)
  			file >> X(i, j);
  		file >> Y(i);
  		X(i, m) = 1;
  	}
  
  	return { X, Y };
  }
  ```

- Load data from above function (inside `main()`)

  ```c++
  string dataset_path = ".\\Datasets\\1. Linear Regression\\x01.txt";
  auto data = get_data(dataset_path);
  auto X = data.first;
  auto Y = data.second;
  ```

- Calculate SVD Decomposition

  ```c++
  BDCSVD<MatrixXd> decomposed_X(X, ComputeThinU | ComputeThinV);
  auto U = decomposed_X.matrixU();
  auto V = decomposed_X.matrixV();
  auto SigmaInv = decomposed_X.singularValues().cwiseInverse().asDiagonal().toDenseMatrix();
  SigmaInv.conservativeResize(U.cols(), V.rows());
  for (int i = V.rows(); i < SigmaInv.rows(); ++i)
  	SigmaInv.row(i).setZero();
  ```

- Compute the value of $w$

  ```c++
  auto result = V.transpose().inverse() * SigmaInv.transpose() * U.transpose() * Y;
  ```

- Predict the results

  ```c++
  auto predicted = X * result;
  ```

- Display Summary

  ```c++
  cout << "Equation Params:\n" << result << endl;
  cout << "\nDiff: \n" << predicted - Y << endl;
  cout << "\nMean Error: " << ((predicted - Y).transpose() * (predicted - Y)) / Y.size() << endl;
  ```

## Datasets

- [Linear Regression Datasets](https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html)