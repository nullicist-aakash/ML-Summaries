# Gradient Descent

## Direction of Steepest Ascent/Descent of a function

### Points to Remember

1. At any point on a surface in 2D (for visualization purposes only), the direction we want to seek is such that if we move in that direction by a unit length, we achieve the greatest change in $f(x,y)$: either increase or decrease.
2. At a point, there is a disc of directions (Domain) from which we have to choose the direction of maximum ascent/descent. How can someone relate the gradient of $f$ at any point to the direction of steepest ascent?

### Resources

1. [Robert Kwiatkowski](https://robertkwiatkowski01.medium.com), [Gradient Descent Algorithm â€” a deep dive](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)
2. [3Blue1Brown](https://www.3blue1brown.com/lessons/gradient-descent)
3. Proof that gradient is actually the direction of maximum ascent: [Gradient  of a function as the direction of steepest ascent/descent](https://math.stackexchange.com/q/1912697)

## Gradient Descent Algorithm

### Points to Remember

- This is a numerical approximation algorithm.
- In theory, if function is convex shaped and we start from from point, we can move in the direction of steepest descent to reach the global minimum, with step size being infinitesimally small.

### Formula

$$
x^{(i+1)}=x^{(i)}-\alpha^{(i)} \left(\frac{\partial f}{\partial x} \right)_{x=x^{(i)}}
$$

$$
\alpha^{(i+1)}=g(\alpha^{(i)},\ i,\ N)
$$

where $g$ represents the update rule for learning rate parameter $\alpha$.

### C++ Algorithm

- Include header files

  ```C++
  #include <iostream>
  #include <vector>
  #include <functional>
  #include <cassert>
  
  using namespace std;
  ```

- Overload operators for vector

  ```C++
  template<class T>
  vector<T>& operator-=(vector<T>& A, const vector<T>& B)
  {
  	assert(A.size() == B.size());
  	for (int i = 0; i < A.size(); ++i)
  		A[i] -= B[i];
  	return A;
  }
  
  template<class T>
  vector<T> operator*(double d, const vector<T>& B)
  {
  	auto temp = B;
  	for (auto& x : temp)
  		x *= d;
  	return std::move(temp);
  }
  ```

- Class for Gradient Descent

  ```C++
  class GD
  {
  	int current_step;
  	double learning_rate;
  	vector<double> current_point;
  	function<vector<double>(vector<double>)> gradient;
  public:
  	const int total_steps;
  
  	GD(double alpha, 
  		int count, 
  		vector<double> initial,
  		function<vector<double>(const vector<double>&)> gradient) :
  		learning_rate{ alpha },
  		total_steps{ count },
  		current_point{ std::move(initial) },
  		gradient{ gradient },
  		current_step{ 0 }
  	{
  
  	}
  
  	void current_iter_learning_rate()
  	{
  		// write the logic here to update the learning rate for the current iterate
  	}
  
  	void apply_one_epoch()
  	{
  		assert(iterations_left());
  		current_iter_learning_rate();
  		current_point -= learning_rate * gradient(current_point);
  		++current_step;
  	}
  
  	bool iterations_left() const
  	{
  		return current_step < total_steps;
  	}
  
  	int get_current_step() const
  	{
  		return current_step;
  	}
  
  	const vector<double>& get_current_point() const
  	{
  		return current_point;
  	}
  };
  ```

- Sample gradient Function

  ```C++
  vector<double> gradient(const vector<double>& Point)
  {
  	// Gradient for z = (x-1)^2 + (y-5)^2: paraboloid
  	assert(Point.size() == 2);
  	return { 2 * Point[0] - 2, 2 * Point[1] - 10 };
  }
  ```

- `main()`

  ```c++
  GD gd(0.01, 1000, { 10, 30 }, gradient);
  while (gd.iterations_left())
  {
      gd.apply_one_epoch();
      if (gd.get_current_step() % 50)
          continue;
  
      cout << "Iteration " << gd.get_current_step() << ": ";
      for (auto& x : gd.get_current_point())
          cout << x << " ";
      cout << endl;
  }
  ```

- Output

  ```bash
  Iteration 50: 4.27753 14.1042
  Iteration 100: 2.19358 8.31549
  Iteration 150: 1.43466 6.2074
  Iteration 200: 1.15829 5.4397
  Iteration 250: 1.05764 5.16012
  Iteration 300: 1.02099 5.05831
  Iteration 350: 1.00764 5.02124
  Iteration 400: 1.00278 5.00773
  Iteration 450: 1.00101 5.00282
  Iteration 500: 1.00037 5.00103
  Iteration 550: 1.00013 5.00037
  Iteration 600: 1.00005 5.00014
  Iteration 650: 1.00002 5.00005
  Iteration 700: 1.00001 5.00002
  Iteration 750: 1 5.00001
  Iteration 800: 1 5
  Iteration 850: 1 5
  Iteration 900: 1 5
  Iteration 950: 1 5
  Iteration 1000: 1 5
  ```

  