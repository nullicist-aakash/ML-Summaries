# Gradient Descent for Linear Regression

Refer previous section for Gradient Descent Algorithm in general.

## Resources

1. [The Hundred-Page Machine Learning Book](http://themlbook.com/wiki/doku.php)
   - [Chapter 4 – Anatomy of a Learning Algorithm](http://bit.ly/theMLbook-Chapter-4): Pages 3 to 8
2. [Stochastic Gradient Descent Algorithm](https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent)

## Derivation

We know that for a convex function $f$ and initial vector $x^{(0)}$ and step size $\alpha^{(i)}$ for $i^{th}$ iteration, we have
$$
x^{(i+1)}=x^{(i)}-\alpha^{(i)} \left(\frac{\partial f}{\partial x} \right)_{x=x^{(i)}}
$$

1. For linear regression, prove that error function is convex.

   - [Solution](https://math.stackexchange.com/a/486234/777826)

2. Prove that gradient descent equation for $W$​ given by
   $$
   W^{(i+1)}=W^{(i)}-\frac{2\alpha^{(i)}}{N}\left(X^TXW^{(i)}-X^TY\right)
   $$

3. Prove that for stochastic gradient descent, update equation is given by
   $$
   W^{(i+1)}=W^{(i)}-2\alpha^{(i)} x^{(i)}\left(x^{(i)}W^{(i)}-y^{(i)}\right)
   $$
   where $x^{(i)}$ and $y^{(i)}$ are data points taken in $i^{th}$ iteration of stochastic gradient descent.

## Implementation in C++

